ğŸ“„ğŸ” RAG-based Document Q&A (Offline)

A fully **offline**, **privacy-friendly** Question & Answer system powered by **Retrieval-Augmented Generation (RAG)** using:

- **Ollama** for local LLM inference  
- **ChromaDB** for blazing-fast vector search  
- **Sentence-Transformers** for high-quality embeddings  

Ask questions about your documents **without sending anything to the cloud**.

---

## ğŸš€ Features

- ğŸ” **100% Offline** â€” All processing happens locally  
- ğŸ“ **Multi-Document Support** â€” Load PDFs, text files, or knowledge dumps  
- ğŸ§  **RAG Pipeline** â€” Retrieve relevant chunks + generate precise answers  
- âš¡ **Fast Vector Search** with ChromaDB  
- ğŸ“¦ **Simple Setup**, minimal dependencies  
- ğŸ› ï¸ Works on CPU and GPU depending on your Ollama model  

---

## ğŸ§± Architecture Overview

```

Documents â†’ Chunking â†’ Embeddings (Sentence-Transformers)
â†“
ChromaDB (Vector Store)
â†“
Query â†’ Retrieve Top-k Chunks â†’ Ollama LLM â†’ Answer

```

This hybrid approach ensures answers are accurate, grounded, and private.


## ğŸ“¥ Installation

### 1ï¸âƒ£ Install Ollama  
Download from: https://ollama.com/download  
Example model (customizable):  
```

ollama pull llama3

```

### 2ï¸âƒ£ Install Python Dependencies  
```

pip install chromadb sentence-transformers

```

### 3ï¸âƒ£ Run the App  
```

python main.py

`ğŸ“š How It Works

1. **Load documents** â†’ Automatically chunked and embedded  
2. **Store vectors** in ChromaDB  
3. **Ask a question** â†’ System fetches best-matching chunks  
4. **Ollama generates an answer** grounded in your documents  

No internet. No cloud APIs. Complete privacy.

ğŸ“ Example Query

**You ask:**  
> â€œSummarize the key points from Chapter 4.â€

**The system:**  
- Finds relevant embeddings  
- Retrieves the top chunks  
- Uses Ollama to generate a grounded, contextual answer  


## ğŸ“‚ Project Structure
ğŸ“¦ rag-doc-qa
â”£ ğŸ“ data/            # Your documents
â”£ ğŸ“ db/              # Local ChromaDB store
â”£ ğŸ“œ app.py           # Main RAG interface
â”£ ğŸ“œ ingest.py         # Embedding + ingestion scripts
â”— ğŸ“œ README.md

ğŸ”® Future Enhancements
- ğŸ–¥ï¸ Web UI (Streamlit / Gradio)  
- ğŸ“‘ PDF smart segmentation  
- ğŸ—‚ï¸ Document tagging & metadata search  
- ğŸ™ï¸ Voice-based Q&A  

ğŸ¤ Contributions
Contributions, issues, and feature requests are welcome!  
Feel free to **open an issue** or submit a **pull request**.
â­ Support
If you found this useful, please consider giving the repo a â­ on GitHub â€” it helps a lot!
